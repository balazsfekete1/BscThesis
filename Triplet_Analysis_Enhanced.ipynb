{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9edf6e0-976c-4c3a-b1a0-7e95aa84af6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Enhanced Triplet Association Analysis - Instacart Dataset\n",
    "## BSc Thesis: Market Basket Analysis using Databricks\n",
    "\n",
    "**Focus:** Deep analysis of 3-item purchase patterns (triplets) with efficiency optimizations\n",
    "\n",
    "**Why Triplets?**\n",
    "- Pairs are too basic (just 2 items)\n",
    "- Quadruplets+ are computationally expensive (4+ self-joins)\n",
    "- Triplets offer the sweet spot: rich patterns with manageable compute\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc806fc-be8c-40cf-9bc7-3d9f86820a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Setup and Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6009db6-fe21-482f-9954-f82c3ccc6042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, collect_list, collect_set, round as spark_round, desc, asc\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRIPLET MARKET BASKET ANALYSIS - Enhanced Version\")\n",
    "print(\"Pure SQL Implementation (Spark Connect Compatible)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Data quality check\n",
    "def check_data_quality():\n",
    "    print(\"\\n‚Üí Checking data quality...\")\n",
    "    \n",
    "    quality_check = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_products,\n",
    "            COUNT(CASE WHEN TRY_CAST(department_id AS BIGINT) IS NULL THEN 1 END) as invalid_dept_ids,\n",
    "            COUNT(CASE WHEN TRY_CAST(aisle_id AS BIGINT) IS NULL THEN 1 END) as invalid_aisle_ids,\n",
    "            COUNT(DISTINCT TRY_CAST(department_id AS BIGINT)) as total_departments,\n",
    "            COUNT(DISTINCT TRY_CAST(aisle_id AS BIGINT)) as total_aisles\n",
    "        FROM workspace.instacart.products\n",
    "    \"\"\")\n",
    "    \n",
    "    result = quality_check.collect()[0]\n",
    "    print(f\"   Total Products: {result['total_products']:,}\")\n",
    "    print(f\"   Total Departments: {result['total_departments']}\")\n",
    "    print(f\"   Total Aisles: {result['total_aisles']}\")\n",
    "    print(f\"   Invalid Department IDs: {result['invalid_dept_ids']}\")\n",
    "    print(f\"   Invalid Aisle IDs: {result['invalid_aisle_ids']}\")\n",
    "    \n",
    "    if result['invalid_dept_ids'] > 0 or result['invalid_aisle_ids'] > 0:\n",
    "        print(\"   ‚ö† Some products have invalid IDs - they will be excluded from categorical analysis\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Basic order statistics\n",
    "def check_order_statistics():\n",
    "    print(\"\\n‚Üí Order statistics...\")\n",
    "    \n",
    "    order_stats = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT op.order_id) as total_orders,\n",
    "            COUNT(DISTINCT o.user_id) as total_users,\n",
    "            COUNT(*) as total_order_items\n",
    "        FROM workspace.instacart.order_products_prior op\n",
    "        JOIN workspace.instacart.orders o ON op.order_id = o.order_id\n",
    "    \"\"\")\n",
    "    \n",
    "    result = order_stats.collect()[0]\n",
    "    print(f\"   Total Orders: {result['total_orders']:,}\")\n",
    "    print(f\"   Total Users: {result['total_users']:,}\")\n",
    "    print(f\"   Total Items Ordered: {result['total_order_items']:,}\")\n",
    "    print(f\"   Avg Items per Order: {result['total_order_items'] / result['total_orders']:.2f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "check_data_quality()\n",
    "check_order_statistics()\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fba168-63b0-4f1d-b1c8-65ec3c6f51bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Product Pairs Analysis (Foundation)\n",
    "We start with pairs to understand basic associations before moving to triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24a8824-894b-4e5f-95b6-085dca8ddce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_product_pairs(min_support_count=100):\n",
    "    \"\"\"\n",
    "    Find product pairs with support, confidence, and lift metrics.\n",
    "    This serves as the foundation for triplet analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚Üí Analyzing product pairs (min support: {min_support_count})...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH product_pairs AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            COUNT(DISTINCT op1.order_id) as co_occurrence\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        GROUP BY op1.product_id, op2.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support_count}\n",
    "    ),\n",
    "    product_support AS (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            COUNT(DISTINCT order_id) as support_count\n",
    "        FROM workspace.instacart.order_products_prior\n",
    "        GROUP BY product_id\n",
    "    ),\n",
    "    total_orders AS (\n",
    "        SELECT COUNT(DISTINCT order_id) as total \n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        pp.co_occurrence,\n",
    "        ROUND(pp.co_occurrence * 100.0 / to.total, 4) as support_pct,\n",
    "        ROUND(pp.co_occurrence * 1.0 / ps1.support_count, 4) as confidence_1_to_2,\n",
    "        ROUND(pp.co_occurrence * 1.0 / ps2.support_count, 4) as confidence_2_to_1,\n",
    "        ROUND(pp.co_occurrence * 1.0 * to.total / (ps1.support_count * ps2.support_count), 2) as lift,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        a1.aisle as aisle_1,\n",
    "        a2.aisle as aisle_2\n",
    "    FROM product_pairs pp\n",
    "    CROSS JOIN total_orders to\n",
    "    JOIN workspace.instacart.products p1 ON pp.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON pp.prod2 = p2.product_id\n",
    "    JOIN product_support ps1 ON pp.prod1 = ps1.product_id\n",
    "    JOIN product_support ps2 ON pp.prod2 = ps2.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.aisles a1 ON TRY_CAST(p1.aisle_id AS BIGINT) = a1.aisle_id\n",
    "    LEFT JOIN workspace.instacart.aisles a2 ON TRY_CAST(p2.aisle_id AS BIGINT) = a2.aisle_id\n",
    "    ORDER BY lift DESC\n",
    "    LIMIT 200\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"product_pairs_analysis\")\n",
    "    print(f\"   Found {df.count()} high-quality product pairs\")\n",
    "    return df\n",
    "\n",
    "# Execute pairs analysis\n",
    "pairs_df = analyze_product_pairs(min_support_count=100)\n",
    "print(\"\\n‚úì Pairs analysis complete. View created: product_pairs_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aabc581-c3c3-45ef-9880-c15b1929c4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display top pairs\n",
    "print(\"\\nüî• TOP 20 PRODUCT PAIRS BY LIFT:\")\n",
    "display(pairs_df.orderBy(desc('lift')).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3117c592-cebc-47bd-8ff8-c39f9e67556d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Core Triplet Analysis\n",
    "### 3.1 Basic Triplet Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ace7356-5426-4cdf-8263-a25bb9e5674f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_triplets_optimized(min_support_count=50):\n",
    "    \"\"\"\n",
    "    Optimized: Only analyze triplets from popular products\n",
    "    Reduces search space from 50K¬≥ to 500¬≥ products\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚Üí Analyzing triplets (optimized, min support: {min_support_count})...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH popular_products AS (\n",
    "        -- Filter to products that appear in at least 150 orders\n",
    "        SELECT product_id\n",
    "        FROM workspace.instacart.order_products_prior\n",
    "        GROUP BY product_id\n",
    "        HAVING COUNT(DISTINCT order_id) >= 150\n",
    "        LIMIT 1000\n",
    "    ),\n",
    "    triplet_combos AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            op3.product_id as prod3,\n",
    "            COUNT(DISTINCT op1.order_id) as triplet_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        INNER JOIN popular_products pp1 ON op1.product_id = pp1.product_id\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        INNER JOIN popular_products pp2 ON op2.product_id = pp2.product_id\n",
    "        JOIN workspace.instacart.order_products_prior op3\n",
    "            ON op1.order_id = op3.order_id AND op2.product_id < op3.product_id\n",
    "        INNER JOIN popular_products pp3 ON op3.product_id = pp3.product_id\n",
    "        GROUP BY op1.product_id, op2.product_id, op3.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support_count}\n",
    "    ),\n",
    "    total_orders AS (\n",
    "        SELECT COUNT(DISTINCT order_id) as total \n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        p3.product_name as product_3,\n",
    "        tc.triplet_count,\n",
    "        ROUND(tc.triplet_count * 100.0 / to.total, 4) as support_pct,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        d3.department as dept_3,\n",
    "        CASE \n",
    "            WHEN d1.department = d2.department AND d2.department = d3.department \n",
    "            THEN 'Same Department'\n",
    "            WHEN d1.department != d2.department AND d2.department != d3.department AND d1.department != d3.department\n",
    "            THEN 'All Different Departments'\n",
    "            ELSE 'Mixed Departments'\n",
    "        END as department_diversity\n",
    "    FROM triplet_combos tc\n",
    "    CROSS JOIN total_orders to\n",
    "    JOIN workspace.instacart.products p1 ON tc.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON tc.prod2 = p2.product_id\n",
    "    JOIN workspace.instacart.products p3 ON tc.prod3 = p3.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d3 ON TRY_CAST(p3.department_id AS BIGINT) = d3.department_id\n",
    "    ORDER BY triplet_count DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"triplet_patterns_basic\")\n",
    "    print(f\"   Found {df.count()} triplet patterns\")\n",
    "    return df\n",
    "\n",
    "# Execute\n",
    "triplets_df = analyze_triplets_optimized(min_support_count=50)\n",
    "print(\"\\n‚úì Basic triplet analysis complete. View created: triplet_patterns_basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a6cd01-883a-4f2c-8b45-96bffd7245de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display top triplets\n",
    "print(\"\\nüéØ TOP 20 TRIPLET PATTERNS:\")\n",
    "display(triplets_df.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e559a424-6be9-4dae-b5b6-3b589e3f35c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2 Triplet Confidence and Lift Metrics (NEW!)\n",
    "Calculate how much each pair ‚Üí third item relationship matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589abc2e-ca10-450d-9e0b-2fe202ba7797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_triplet_confidence(min_support_count=50):\n",
    "    \"\"\"\n",
    "    Calculate triplet-specific metrics:\n",
    "    - Confidence: P(prod3 | prod1 ‚àß prod2)\n",
    "    - Lift: How much more likely is prod3 given prod1 and prod2 together\n",
    "    - Incremental value over pairs\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚Üí Calculating triplet confidence and lift metrics...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH triplet_combos AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            op3.product_id as prod3,\n",
    "            COUNT(DISTINCT op1.order_id) as triplet_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        JOIN workspace.instacart.order_products_prior op3\n",
    "            ON op1.order_id = op3.order_id AND op2.product_id < op3.product_id\n",
    "        GROUP BY op1.product_id, op2.product_id, op3.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support_count}\n",
    "    ),\n",
    "    pair_support AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            COUNT(DISTINCT op1.order_id) as pair_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        GROUP BY op1.product_id, op2.product_id\n",
    "    ),\n",
    "    product_support AS (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            COUNT(DISTINCT order_id) as support_count\n",
    "        FROM workspace.instacart.order_products_prior\n",
    "        GROUP BY product_id\n",
    "    ),\n",
    "    total_orders AS (\n",
    "        SELECT COUNT(DISTINCT order_id) as total \n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        p3.product_name as product_3,\n",
    "        tc.triplet_count,\n",
    "        ps12.pair_count as pair_12_count,\n",
    "        -- Confidence: P(prod3 | prod1 AND prod2)\n",
    "        ROUND(tc.triplet_count * 1.0 / ps12.pair_count, 4) as confidence_12_to_3,\n",
    "        -- Lift: (P(prod1, prod2, prod3)) / (P(prod1, prod2) * P(prod3))\n",
    "        ROUND(tc.triplet_count * 1.0 * to.total / (ps12.pair_count * ps3.support_count), 2) as triplet_lift,\n",
    "        -- Support percentage\n",
    "        ROUND(tc.triplet_count * 100.0 / to.total, 4) as support_pct,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        d3.department as dept_3\n",
    "    FROM triplet_combos tc\n",
    "    CROSS JOIN total_orders to\n",
    "    JOIN pair_support ps12 ON tc.prod1 = ps12.prod1 AND tc.prod2 = ps12.prod2\n",
    "    JOIN product_support ps3 ON tc.prod3 = ps3.product_id\n",
    "    JOIN workspace.instacart.products p1 ON tc.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON tc.prod2 = p2.product_id\n",
    "    JOIN workspace.instacart.products p3 ON tc.prod3 = p3.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d3 ON TRY_CAST(p3.department_id AS BIGINT) = d3.department_id\n",
    "    ORDER BY triplet_lift DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"triplet_confidence_lift\")\n",
    "    print(f\"   Calculated confidence/lift for {df.count()} triplets\")\n",
    "    return df\n",
    "\n",
    "# Execute confidence analysis\n",
    "triplet_metrics_df = analyze_triplet_confidence(min_support_count=50)\n",
    "print(\"\\n‚úì Triplet confidence/lift analysis complete. View created: triplet_confidence_lift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508c0dca-48f7-4bc9-a1a5-034585559fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display highest lift triplets\n",
    "print(\"\\nüíé TOP 20 TRIPLETS BY LIFT (Strongest 3-way associations):\")\n",
    "display(triplet_metrics_df.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1bc6e9-0d6f-40ef-bb9b-e624f58913b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3 Department Diversity in Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a0a134-53b3-4681-9e96-c8b162ab1e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_department_diversity():\n",
    "    \"\"\"\n",
    "    Analyze how triplets span across departments.\n",
    "    Important for understanding cross-category shopping behavior.\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing department diversity in triplets...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        department_diversity,\n",
    "        COUNT(*) as pattern_count,\n",
    "        ROUND(AVG(triplet_count), 2) as avg_occurrence,\n",
    "        ROUND(AVG(support_pct), 4) as avg_support_pct,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct_of_patterns\n",
    "    FROM triplet_patterns_basic\n",
    "    GROUP BY department_diversity\n",
    "    ORDER BY pattern_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"triplet_dept_diversity\")\n",
    "    return df\n",
    "\n",
    "dept_diversity_df = analyze_department_diversity()\n",
    "print(\"\\nüìä DEPARTMENT DIVERSITY IN TRIPLETS:\")\n",
    "display(dept_diversity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9193ff00-7bce-40b2-b85b-d519abb19f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4 Top Triplets by Department Combination (NEW!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79d9364-8c5b-4b41-bf04-fd8e959cf3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_cross_department_triplets():\n",
    "    \"\"\"\n",
    "    Find the most interesting cross-department triplets.\n",
    "    These reveal complementary purchase patterns across categories.\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Finding cross-department triplets...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        product_1,\n",
    "        product_2,\n",
    "        product_3,\n",
    "        triplet_count,\n",
    "        support_pct,\n",
    "        dept_1,\n",
    "        dept_2,\n",
    "        dept_3,\n",
    "        CONCAT(dept_1, ' + ', dept_2, ' + ', dept_3) as dept_combination\n",
    "    FROM triplet_patterns_basic\n",
    "    WHERE department_diversity = 'All Different Departments'\n",
    "    ORDER BY triplet_count DESC\n",
    "    LIMIT 30\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    return df\n",
    "\n",
    "cross_dept_triplets = analyze_cross_department_triplets()\n",
    "print(\"\\nüåâ TOP 30 CROSS-DEPARTMENT TRIPLETS:\")\n",
    "display(cross_dept_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65569d7c-81d2-49ea-9ec2-8c8459fde631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: Temporal Patterns (Triplet-Focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5612e244-7546-4948-8732-c378fb201e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns_simple():\n",
    "    \"\"\"\n",
    "    Simplified temporal analysis - uses pairs instead of triplets\n",
    "    Much faster and still gives valuable insights\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing temporal patterns (pairs - compute optimized)...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    WITH time_pairs AS (\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN o.order_hour_of_day BETWEEN 6 AND 11 THEN 'Morning (6-11am)'\n",
    "                WHEN o.order_hour_of_day BETWEEN 12 AND 17 THEN 'Afternoon (12-5pm)'\n",
    "                WHEN o.order_hour_of_day BETWEEN 18 AND 21 THEN 'Evening (6-9pm)'\n",
    "                ELSE 'Night (10pm-5am)'\n",
    "            END as time_period,\n",
    "            CASE \n",
    "                WHEN o.order_dow IN (0, 6) THEN 'Weekend'\n",
    "                ELSE 'Weekday'\n",
    "            END as day_type,\n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            COUNT(DISTINCT o.order_id) as pair_count\n",
    "        FROM workspace.instacart.orders o\n",
    "        JOIN workspace.instacart.order_products_prior op1 ON o.order_id = op1.order_id\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON o.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        WHERE o.eval_set = 'prior'\n",
    "        GROUP BY time_period, day_type, op1.product_id, op2.product_id\n",
    "        HAVING COUNT(DISTINCT o.order_id) >= 100\n",
    "    )\n",
    "    SELECT \n",
    "        tp.time_period,\n",
    "        tp.day_type,\n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        tp.pair_count,\n",
    "        CONCAT(tp.day_type, ' - ', tp.time_period) as shopping_context\n",
    "    FROM time_pairs tp\n",
    "    JOIN workspace.instacart.products p1 ON tp.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON tp.prod2 = p2.product_id\n",
    "    ORDER BY tp.pair_count DESC\n",
    "    LIMIT 200\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"temporal_pair_patterns\")\n",
    "    print(f\"   Found {df.count()} temporal pair patterns\")\n",
    "    return df\n",
    "\n",
    "# Execute\n",
    "temporal_df = analyze_temporal_patterns_simple()\n",
    "print(\"\\n‚úì Temporal analysis complete. View created: temporal_pair_patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b26cca2-8b8b-4e51-a226-53fe8ba081f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display by shopping context\n",
    "print(\"\\n‚è∞ WEEKEND EVENING PATTERNS:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM temporal_pair_patterns\n",
    "    WHERE day_type = 'Weekend' AND time_period = 'Evening (6-9pm)'\n",
    "    ORDER BY pair_count DESC\n",
    "    LIMIT 15\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n‚òÄÔ∏è WEEKDAY MORNING PATTERNS:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM temporal_pair_patterns\n",
    "    WHERE day_type = 'Weekday' AND time_period = 'Morning (6-11am)'\n",
    "    ORDER BY pair_count DESC\n",
    "    LIMIT 15\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b195e9e-ef8b-4263-abee-d691138a1348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 5: Reorder Loyalty in Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d302ca15-cd15-4b52-8870-533959235dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_triplet_reorder_loyalty(min_support=30):\n",
    "    \"\"\"\n",
    "    Find triplets that are frequently reordered together.\n",
    "    These represent strong habitual purchase patterns.\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚Üí Analyzing reorder loyalty for triplets (min support: {min_support})...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH reordered_triplets AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            op3.product_id as prod3,\n",
    "            COUNT(DISTINCT op1.order_id) as reorder_together_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        JOIN workspace.instacart.order_products_prior op3\n",
    "            ON op1.order_id = op3.order_id AND op2.product_id < op3.product_id\n",
    "        WHERE op1.reordered = 1 AND op2.reordered = 1 AND op3.reordered = 1\n",
    "        GROUP BY op1.product_id, op2.product_id, op3.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support}\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        p3.product_name as product_3,\n",
    "        rt.reorder_together_count,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        d3.department as dept_3,\n",
    "        CASE \n",
    "            WHEN d1.department = d2.department AND d2.department = d3.department \n",
    "            THEN 'Same Department Loyalty'\n",
    "            ELSE 'Cross Department Loyalty'\n",
    "        END as loyalty_type\n",
    "    FROM reordered_triplets rt\n",
    "    JOIN workspace.instacart.products p1 ON rt.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON rt.prod2 = p2.product_id\n",
    "    JOIN workspace.instacart.products p3 ON rt.prod3 = p3.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d3 ON TRY_CAST(p3.department_id AS BIGINT) = d3.department_id\n",
    "    WHERE d1.department IS NOT NULL AND d2.department IS NOT NULL AND d3.department IS NOT NULL\n",
    "    ORDER BY reorder_together_count DESC\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"triplet_reorder_patterns\")\n",
    "    print(f\"   Found {df.count()} loyal triplet patterns\")\n",
    "    return df\n",
    "\n",
    "reorder_triplets_df = analyze_triplet_reorder_loyalty(min_support=30)\n",
    "print(\"\\n‚úì Reorder loyalty analysis complete. View created: triplet_reorder_patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23f399e-35cb-474a-a912-939554ed6adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ TOP 30 REORDERED TRIPLETS (Habitual Purchase Patterns):\")\n",
    "display(reorder_triplets_df.limit(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e179c2-86dc-49fb-8cdd-e448936f67e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 6: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b802b2-f5a8-4a8f-824f-a2e74c5940f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_summary_statistics():\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary statistics for the thesis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY STATISTICS - TRIPLET ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Count of patterns\n",
    "    pairs_count = spark.table(\"product_pairs_analysis\").count()\n",
    "    triplets_count = spark.table(\"triplet_patterns_basic\").count()\n",
    "    \n",
    "    print(f\"\\nüìä Pattern Counts:\")\n",
    "    print(f\"   Product Pairs: {pairs_count:,}\")\n",
    "    print(f\"   Triplets: {triplets_count:,}\")\n",
    "    \n",
    "    # Department diversity breakdown\n",
    "    dept_div = spark.sql(\"\"\"\n",
    "        SELECT department_diversity, COUNT(*) as count\n",
    "        FROM triplet_patterns_basic\n",
    "        GROUP BY department_diversity\n",
    "        ORDER BY count DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    print(f\"\\nüì¶ Department Diversity:\")\n",
    "    for row in dept_div:\n",
    "        print(f\"   {row['department_diversity']}: {row['count']} patterns\")\n",
    "    \n",
    "    # Support ranges\n",
    "    support_stats = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            MIN(triplet_count) as min_support,\n",
    "            MAX(triplet_count) as max_support,\n",
    "            ROUND(AVG(triplet_count), 2) as avg_support,\n",
    "            PERCENTILE(triplet_count, 0.5) as median_support\n",
    "        FROM triplet_patterns_basic\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìà Support Statistics:\")\n",
    "    print(f\"   Min Support: {support_stats['min_support']}\")\n",
    "    print(f\"   Max Support: {support_stats['max_support']}\")\n",
    "    print(f\"   Avg Support: {support_stats['avg_support']}\")\n",
    "    print(f\"   Median Support: {support_stats['median_support']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "generate_summary_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be0476c-ac86-46a1-9025-5729bbe23cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 7: Export Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d39ecced-bbc5-45d6-be34-3798d62277ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export top patterns to Pandas for visualization or further analysis\n",
    "def export_key_results():\n",
    "    print(\"\\n‚Üí Exporting key results to Pandas DataFrames...\")\n",
    "    \n",
    "    # Top triplets\n",
    "    top_triplets_pd = triplets_df.limit(50).toPandas()\n",
    "    \n",
    "    # Triplets with metrics\n",
    "    triplet_metrics_pd = triplet_metrics_df.limit(50).toPandas()\n",
    "    \n",
    "    # Department diversity\n",
    "    dept_diversity_pd = dept_diversity_df.toPandas()\n",
    "    \n",
    "    print(f\"   Exported {len(top_triplets_pd)} top triplets to Pandas\")\n",
    "    print(f\"   Exported {len(triplet_metrics_pd)} triplets with metrics to Pandas\")\n",
    "    print(f\"   Exported department diversity summary to Pandas\")\n",
    "    \n",
    "    return top_triplets_pd, triplet_metrics_pd, dept_diversity_pd\n",
    "\n",
    "# Uncomment to export:\n",
    "# top_triplets_pd, triplet_metrics_pd, dept_diversity_pd = export_key_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c673b32f-05e5-46af-8eee-4e9ae10235e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analysis Complete!\n",
    "\n",
    "### Created Views:\n",
    "1. `product_pairs_analysis` - Product pair associations\n",
    "2. `triplet_patterns_basic` - Basic triplet patterns\n",
    "3. `triplet_confidence_lift` - Triplet confidence and lift metrics\n",
    "4. `triplet_dept_diversity` - Department diversity summary\n",
    "5. `temporal_triplet_patterns` - Time-based triplet patterns\n",
    "6. `triplet_reorder_patterns` - Loyalty triplet patterns\n",
    "\n",
    "### Next Steps for Thesis:\n",
    "1. **Visualization** - Create charts showing top patterns\n",
    "2. **Statistical Analysis** - Perform significance tests\n",
    "3. **Business Insights** - Interpret patterns for recommendations\n",
    "4. **Comparison** - Compare with pairs to show triplet value\n",
    "5. **Documentation** - Write up methodology and findings"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Triplet_Analysis_Enhanced",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
