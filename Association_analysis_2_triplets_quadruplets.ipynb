{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b9da97-5e64-40a8-9e44-230cb25a1514",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762258194693}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762258034021}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENHANCED INSTACART ASSOCIATION ANALYSIS\n",
    "# Pure SQL Implementation (No ML Library - Spark Connect Compatible)\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import col, count, collect_list, collect_set, round as spark_round\n",
    "import pandas as pd\n",
    "\n",
    "# NOTE: This notebook uses PURE SQL queries - no Spark ML libraries\n",
    "# Compatible with Databricks Spark Connect\n",
    "\n",
    "print(\"Starting Market Basket Analysis - Pure SQL Implementation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# DATA QUALITY CHECK\n",
    "# ============================================\n",
    "\n",
    "def check_data_quality():\n",
    "    \"\"\"\n",
    "    Check for data quality issues in the products table\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Checking data quality...\")\n",
    "    \n",
    "    quality_check = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_products,\n",
    "            COUNT(CASE WHEN TRY_CAST(department_id AS BIGINT) IS NULL THEN 1 END) as invalid_dept_ids,\n",
    "            COUNT(CASE WHEN TRY_CAST(aisle_id AS BIGINT) IS NULL THEN 1 END) as invalid_aisle_ids,\n",
    "            COUNT(CASE WHEN TRY_CAST(department_id AS BIGINT) IS NOT NULL \n",
    "                       AND TRY_CAST(aisle_id AS BIGINT) IS NOT NULL THEN 1 END) as valid_products\n",
    "        FROM workspace.instacart.products\n",
    "    \"\"\")\n",
    "    \n",
    "    result = quality_check.collect()[0]\n",
    "    print(f\"   Total Products: {result['total_products']}\")\n",
    "    print(f\"   Invalid Department IDs: {result['invalid_dept_ids']}\")\n",
    "    print(f\"   Invalid Aisle IDs: {result['invalid_aisle_ids']}\")\n",
    "    print(f\"   Valid Products: {result['valid_products']}\")\n",
    "    \n",
    "    if result['invalid_dept_ids'] > 0 or result['invalid_aisle_ids'] > 0:\n",
    "        print(\"   ‚ö† Some products have invalid IDs - they will be excluded from department/aisle analysis\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run data quality check\n",
    "check_data_quality()\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PART 1: HIGH-IMPACT PRODUCT PAIRS\n",
    "# ============================================\n",
    "\n",
    "def analyze_product_pairs(min_support_count=100):\n",
    "    \"\"\"\n",
    "    Find product pairs with support, confidence, and lift metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing product pairs...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH product_pairs AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            COUNT(DISTINCT op1.order_id) as co_occurrence\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        GROUP BY op1.product_id, op2.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support_count}\n",
    "    ),\n",
    "    product_support AS (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            COUNT(DISTINCT order_id) as support_count\n",
    "        FROM workspace.instacart.order_products_prior\n",
    "        GROUP BY product_id\n",
    "    ),\n",
    "    total_orders AS (\n",
    "        SELECT COUNT(DISTINCT order_id) as total \n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        pp.co_occurrence,\n",
    "        ROUND(pp.co_occurrence * 100.0 / to.total, 4) as support_pct,\n",
    "        ROUND(pp.co_occurrence * 1.0 / ps1.support_count, 4) as confidence_1_to_2,\n",
    "        ROUND(pp.co_occurrence * 1.0 / ps2.support_count, 4) as confidence_2_to_1,\n",
    "        ROUND(pp.co_occurrence * 1.0 * to.total / (ps1.support_count * ps2.support_count), 2) as lift,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        a1.aisle as aisle_1,\n",
    "        a2.aisle as aisle_2\n",
    "    FROM product_pairs pp\n",
    "    CROSS JOIN total_orders to\n",
    "    JOIN workspace.instacart.products p1 ON pp.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON pp.prod2 = p2.product_id\n",
    "    JOIN product_support ps1 ON pp.prod1 = ps1.product_id\n",
    "    JOIN product_support ps2 ON pp.prod2 = ps2.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.aisles a1 ON TRY_CAST(p1.aisle_id AS BIGINT) = a1.aisle_id\n",
    "    LEFT JOIN workspace.instacart.aisles a2 ON TRY_CAST(p2.aisle_id AS BIGINT) = a2.aisle_id\n",
    "    ORDER BY lift DESC\n",
    "    LIMIT 200\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"product_pairs_analysis\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 2: TRIPLET ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "def analyze_triplets(min_support_count=50):\n",
    "    \"\"\"\n",
    "    Find 3-item combinations that frequently appear together\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing triplets (3-item sets)...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH triplet_combos AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            op3.product_id as prod3,\n",
    "            COUNT(DISTINCT op1.order_id) as triplet_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        JOIN workspace.instacart.order_products_prior op3\n",
    "            ON op1.order_id = op3.order_id AND op2.product_id < op3.product_id\n",
    "        GROUP BY op1.product_id, op2.product_id, op3.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support_count}\n",
    "    ),\n",
    "    total_orders AS (\n",
    "        SELECT COUNT(DISTINCT order_id) as total \n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        p3.product_name as product_3,\n",
    "        tc.triplet_count,\n",
    "        ROUND(tc.triplet_count * 100.0 / to.total, 4) as support_pct,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        d3.department as dept_3,\n",
    "        CASE \n",
    "            WHEN d1.department = d2.department AND d2.department = d3.department \n",
    "            THEN 'Same Department'\n",
    "            WHEN d1.department != d2.department AND d2.department != d3.department AND d1.department != d3.department\n",
    "            THEN 'All Different Departments'\n",
    "            ELSE 'Mixed Departments'\n",
    "        END as department_diversity\n",
    "    FROM triplet_combos tc\n",
    "    CROSS JOIN total_orders to\n",
    "    JOIN workspace.instacart.products p1 ON tc.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON tc.prod2 = p2.product_id\n",
    "    JOIN workspace.instacart.products p3 ON tc.prod3 = p3.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d3 ON TRY_CAST(p3.department_id AS BIGINT) = d3.department_id\n",
    "    ORDER BY triplet_count DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"triplet_patterns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 3: QUADRUPLET ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "def analyze_quadruplets(min_support_count=30):\n",
    "    \"\"\"\n",
    "    Find 4-item combinations - the most complete purchase patterns\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing quadruplets (4-item sets)...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH quad_combos AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            op3.product_id as prod3,\n",
    "            op4.product_id as prod4,\n",
    "            COUNT(DISTINCT op1.order_id) as quad_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        JOIN workspace.instacart.order_products_prior op3\n",
    "            ON op1.order_id = op3.order_id AND op2.product_id < op3.product_id\n",
    "        JOIN workspace.instacart.order_products_prior op4\n",
    "            ON op1.order_id = op4.order_id AND op3.product_id < op4.product_id\n",
    "        GROUP BY op1.product_id, op2.product_id, op3.product_id, op4.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support_count}\n",
    "    ),\n",
    "    total_orders AS (\n",
    "        SELECT COUNT(DISTINCT order_id) as total \n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        p3.product_name as product_3,\n",
    "        p4.product_name as product_4,\n",
    "        qc.quad_count,\n",
    "        ROUND(qc.quad_count * 100.0 / to.total, 4) as support_pct,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        d3.department as dept_3,\n",
    "        d4.department as dept_4\n",
    "    FROM quad_combos qc\n",
    "    CROSS JOIN total_orders to\n",
    "    JOIN workspace.instacart.products p1 ON qc.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON qc.prod2 = p2.product_id\n",
    "    JOIN workspace.instacart.products p3 ON qc.prod3 = p3.product_id\n",
    "    JOIN workspace.instacart.products p4 ON qc.prod4 = p4.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d3 ON TRY_CAST(p3.department_id AS BIGINT) = d3.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d4 ON TRY_CAST(p4.department_id AS BIGINT) = d4.department_id\n",
    "    ORDER BY quad_count DESC\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"quadruplet_patterns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 4: TEMPORAL PATTERNS\n",
    "# ============================================\n",
    "\n",
    "def analyze_temporal_patterns(min_support=50):\n",
    "    \"\"\"\n",
    "    Shopping patterns by time of day and day of week\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing temporal patterns...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH time_patterns AS (\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN o.order_hour_of_day BETWEEN 6 AND 11 THEN 'Morning (6-11am)'\n",
    "                WHEN o.order_hour_of_day BETWEEN 12 AND 17 THEN 'Afternoon (12-5pm)'\n",
    "                WHEN o.order_hour_of_day BETWEEN 18 AND 21 THEN 'Evening (6-9pm)'\n",
    "                ELSE 'Night (10pm-5am)'\n",
    "            END as time_period,\n",
    "            CASE \n",
    "                WHEN o.order_dow IN (0, 6) THEN 'Weekend'\n",
    "                ELSE 'Weekday'\n",
    "            END as day_type,\n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            COUNT(DISTINCT o.order_id) as pair_count\n",
    "        FROM workspace.instacart.orders o\n",
    "        JOIN workspace.instacart.order_products_prior op1 ON o.order_id = op1.order_id\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON o.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        WHERE o.eval_set = 'prior'\n",
    "        GROUP BY time_period, day_type, op1.product_id, op2.product_id\n",
    "        HAVING COUNT(DISTINCT o.order_id) >= {min_support}\n",
    "    )\n",
    "    SELECT \n",
    "        tp.time_period,\n",
    "        tp.day_type,\n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        tp.pair_count,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        CONCAT(tp.day_type, ' - ', tp.time_period) as shopping_context\n",
    "    FROM time_patterns tp\n",
    "    JOIN workspace.instacart.products p1 ON tp.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON tp.prod2 = p2.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    ORDER BY tp.pair_count DESC\n",
    "    LIMIT 200\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"temporal_patterns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 5: CROSS-DEPARTMENT DISCOVERIES\n",
    "# ============================================\n",
    "\n",
    "def analyze_cross_department_synergies(min_lift=2.5, min_support=30):\n",
    "    \"\"\"\n",
    "    Unexpected cross-department product combinations\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Finding cross-department synergies...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH dept_pairs AS (\n",
    "        SELECT \n",
    "            TRY_CAST(p1.department_id AS BIGINT) as dept1_id,\n",
    "            TRY_CAST(p2.department_id AS BIGINT) as dept2_id,\n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            COUNT(DISTINCT op1.order_id) as pair_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id AND op1.product_id < op2.product_id\n",
    "        JOIN workspace.instacart.products p1 ON op1.product_id = p1.product_id\n",
    "        JOIN workspace.instacart.products p2 ON op2.product_id = p2.product_id\n",
    "        WHERE TRY_CAST(p1.department_id AS BIGINT) IS NOT NULL\n",
    "          AND TRY_CAST(p2.department_id AS BIGINT) IS NOT NULL\n",
    "          AND TRY_CAST(p1.department_id AS BIGINT) != TRY_CAST(p2.department_id AS BIGINT)\n",
    "        GROUP BY TRY_CAST(p1.department_id AS BIGINT), TRY_CAST(p2.department_id AS BIGINT), op1.product_id, op2.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= {min_support}\n",
    "    ),\n",
    "    product_support AS (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            COUNT(DISTINCT order_id) as support\n",
    "        FROM workspace.instacart.order_products_prior\n",
    "        GROUP BY product_id\n",
    "    ),\n",
    "    total_orders AS (\n",
    "        SELECT COUNT(DISTINCT order_id) as total \n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    )\n",
    "    SELECT \n",
    "        d1.department as department_1,\n",
    "        d2.department as department_2,\n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        dp.pair_count,\n",
    "        ROUND(dp.pair_count * 1.0 / ps1.support, 4) as confidence,\n",
    "        ROUND(dp.pair_count * 1.0 * to.total / (ps1.support * ps2.support), 2) as lift\n",
    "    FROM dept_pairs dp\n",
    "    CROSS JOIN total_orders to\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON dp.dept1_id = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON dp.dept2_id = d2.department_id\n",
    "    JOIN workspace.instacart.products p1 ON dp.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON dp.prod2 = p2.product_id\n",
    "    JOIN product_support ps1 ON dp.prod1 = ps1.product_id\n",
    "    JOIN product_support ps2 ON dp.prod2 = ps2.product_id\n",
    "    WHERE ROUND(dp.pair_count * 1.0 * to.total / (ps1.support * ps2.support), 2) > {min_lift}\n",
    "    ORDER BY lift DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"cross_department_insights\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 6: SEQUENTIAL SHOPPING BEHAVIOR\n",
    "# ============================================\n",
    "\n",
    "def analyze_sequential_patterns(min_support=80):\n",
    "    \"\"\"\n",
    "    Order in which products are added to cart\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Mining sequential patterns...\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH ordered_products AS (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            product_id,\n",
    "            add_to_cart_order,\n",
    "            ROW_NUMBER() OVER (PARTITION BY order_id ORDER BY add_to_cart_order) as position\n",
    "        FROM workspace.instacart.order_products_prior\n",
    "    ),\n",
    "    sequences AS (\n",
    "        SELECT \n",
    "            op1.product_id as first_product,\n",
    "            op2.product_id as second_product,\n",
    "            COUNT(*) as sequence_count,\n",
    "            AVG(op1.position) as avg_first_position,\n",
    "            AVG(op2.position) as avg_second_position\n",
    "        FROM ordered_products op1\n",
    "        JOIN ordered_products op2 \n",
    "            ON op1.order_id = op2.order_id \n",
    "            AND op2.position = op1.position + 1\n",
    "        GROUP BY op1.product_id, op2.product_id\n",
    "        HAVING COUNT(*) >= {min_support}\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as first_product,\n",
    "        p2.product_name as second_product,\n",
    "        s.sequence_count,\n",
    "        ROUND(s.avg_first_position, 1) as avg_first_pos,\n",
    "        ROUND(s.avg_second_position, 1) as avg_second_pos,\n",
    "        d1.department as first_dept,\n",
    "        d2.department as second_dept,\n",
    "        a1.aisle as first_aisle,\n",
    "        a2.aisle as second_aisle\n",
    "    FROM sequences s\n",
    "    JOIN workspace.instacart.products p1 ON s.first_product = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON s.second_product = p2.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    LEFT JOIN workspace.instacart.aisles a1 ON TRY_CAST(p1.aisle_id AS BIGINT) = a1.aisle_id\n",
    "    LEFT JOIN workspace.instacart.aisles a2 ON TRY_CAST(p2.aisle_id AS BIGINT) = a2.aisle_id\n",
    "    ORDER BY sequence_count DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"sequential_patterns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 7: BASKET DIVERSITY ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "def analyze_basket_compositions():\n",
    "    \"\"\"\n",
    "    Basket size and diversity metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing basket compositions...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    WITH basket_stats AS (\n",
    "        SELECT \n",
    "            o.order_id,\n",
    "            COUNT(DISTINCT op.product_id) as total_items,\n",
    "            COUNT(DISTINCT p.department_id) as unique_departments,\n",
    "            COUNT(DISTINCT p.aisle_id) as unique_aisles\n",
    "        FROM workspace.instacart.orders o\n",
    "        JOIN workspace.instacart.order_products_prior op ON o.order_id = op.order_id\n",
    "        JOIN workspace.instacart.products p ON op.product_id = p.product_id\n",
    "        WHERE o.eval_set = 'prior'\n",
    "          AND TRY_CAST(p.department_id AS BIGINT) IS NOT NULL\n",
    "          AND TRY_CAST(p.aisle_id AS BIGINT) IS NOT NULL\n",
    "        GROUP BY o.order_id\n",
    "    ),\n",
    "    size_categories AS (\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN total_items <= 5 THEN 'Small (1-5 items)'\n",
    "                WHEN total_items <= 15 THEN 'Medium (6-15 items)'\n",
    "                WHEN total_items <= 30 THEN 'Large (16-30 items)'\n",
    "                ELSE 'XLarge (30+ items)'\n",
    "            END as basket_size,\n",
    "            total_items,\n",
    "            unique_departments,\n",
    "            unique_aisles\n",
    "        FROM basket_stats\n",
    "    )\n",
    "    SELECT \n",
    "        basket_size,\n",
    "        COUNT(*) as basket_count,\n",
    "        ROUND(AVG(total_items), 2) as avg_items,\n",
    "        ROUND(AVG(unique_departments), 2) as avg_departments,\n",
    "        ROUND(AVG(unique_aisles), 2) as avg_aisles,\n",
    "        ROUND(AVG(unique_aisles * 1.0 / total_items), 3) as diversity_ratio,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as pct_of_total\n",
    "    FROM size_categories\n",
    "    GROUP BY basket_size\n",
    "    ORDER BY \n",
    "        CASE basket_size\n",
    "            WHEN 'Small (1-5 items)' THEN 1\n",
    "            WHEN 'Medium (6-15 items)' THEN 2\n",
    "            WHEN 'Large (16-30 items)' THEN 3\n",
    "            ELSE 4\n",
    "        END\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"basket_compositions\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 8: REORDER LOYALTY PATTERNS\n",
    "# ============================================\n",
    "\n",
    "def analyze_reorder_patterns():\n",
    "    \"\"\"\n",
    "    Products frequently reordered together show strong loyalty\n",
    "    \"\"\"\n",
    "    print(\"\\n‚Üí Analyzing reorder patterns...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "    WITH reordered_pairs AS (\n",
    "        SELECT \n",
    "            op1.product_id as prod1,\n",
    "            op2.product_id as prod2,\n",
    "            COUNT(DISTINCT op1.order_id) as reorder_together_count\n",
    "        FROM workspace.instacart.order_products_prior op1\n",
    "        JOIN workspace.instacart.order_products_prior op2 \n",
    "            ON op1.order_id = op2.order_id \n",
    "            AND op1.product_id < op2.product_id\n",
    "        WHERE op1.reordered = 1 AND op2.reordered = 1\n",
    "        GROUP BY op1.product_id, op2.product_id\n",
    "        HAVING COUNT(DISTINCT op1.order_id) >= 100\n",
    "    )\n",
    "    SELECT \n",
    "        p1.product_name as product_1,\n",
    "        p2.product_name as product_2,\n",
    "        rp.reorder_together_count,\n",
    "        d1.department as dept_1,\n",
    "        d2.department as dept_2,\n",
    "        CASE \n",
    "            WHEN d1.department = d2.department THEN 'Same Department Loyalty'\n",
    "            ELSE 'Cross Department Loyalty'\n",
    "        END as loyalty_type\n",
    "    FROM reordered_pairs rp\n",
    "    JOIN workspace.instacart.products p1 ON rp.prod1 = p1.product_id\n",
    "    JOIN workspace.instacart.products p2 ON rp.prod2 = p2.product_id\n",
    "    LEFT JOIN workspace.instacart.departments d1 ON TRY_CAST(p1.department_id AS BIGINT) = d1.department_id\n",
    "    LEFT JOIN workspace.instacart.departments d2 ON TRY_CAST(p2.department_id AS BIGINT) = d2.department_id\n",
    "    WHERE d1.department IS NOT NULL AND d2.department IS NOT NULL\n",
    "    ORDER BY reorder_together_count DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(\"reorder_patterns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXECUTING COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Run all analyses\n",
    "results['pairs'] = analyze_product_pairs(min_support_count=100)\n",
    "results['triplets'] = analyze_triplets(min_support_count=50)\n",
    "results['quadruplets'] = analyze_quadruplets(min_support_count=30)\n",
    "results['temporal'] = analyze_temporal_patterns(min_support=50)\n",
    "results['cross_dept'] = analyze_cross_department_synergies(min_lift=2.5)\n",
    "results['sequences'] = analyze_sequential_patterns(min_support=80)\n",
    "results['compositions'] = analyze_basket_compositions()\n",
    "results['reorders'] = analyze_reorder_patterns()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll temporary views created:\")\n",
    "print(\"  ‚Ä¢ product_pairs_analysis\")\n",
    "print(\"  ‚Ä¢ triplet_patterns\")\n",
    "print(\"  ‚Ä¢ quadruplet_patterns\")\n",
    "print(\"  ‚Ä¢ temporal_patterns\")\n",
    "print(\"  ‚Ä¢ cross_department_insights\")\n",
    "print(\"  ‚Ä¢ sequential_patterns\")\n",
    "print(\"  ‚Ä¢ basket_compositions\")\n",
    "print(\"  ‚Ä¢ reorder_patterns\")\n",
    "\n",
    "# ============================================\n",
    "# DISPLAY KEY RESULTS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"üìä KEY FINDINGS - TOP PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\\nüî• TOP 15 HIGH-LIFT PRODUCT PAIRS (Strongest Associations):\")\n",
    "display(results['pairs'].orderBy(col('lift').desc()).limit(15))\n",
    "\n",
    "print(\"\\n\\nüéØ TOP 15 TRIPLET PATTERNS (3-Item Combos):\")\n",
    "display(results['triplets'].orderBy(col('triplet_count').desc()).limit(15))\n",
    "\n",
    "print(\"\\n\\nüíé TOP 10 QUADRUPLET PATTERNS (4-Item Complete Sets):\")\n",
    "display(results['quadruplets'].orderBy(col('quad_count').desc()).limit(10))\n",
    "\n",
    "print(\"\\n\\nüåâ CROSS-DEPARTMENT DISCOVERIES (Unexpected Pairings):\")\n",
    "display(results['cross_dept'].orderBy(col('lift').desc()).limit(15))\n",
    "\n",
    "print(\"\\n\\n‚è∞ WEEKEND EVENING SHOPPING PATTERNS:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM temporal_patterns \n",
    "    WHERE day_type = 'Weekend' AND time_period = 'Evening (6-9pm)'\n",
    "    ORDER BY pair_count DESC\n",
    "    LIMIT 15\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n\\nüîÑ REORDER LOYALTY PATTERNS (Habitual Purchases):\")\n",
    "display(results['reorders'].orderBy(col('reorder_together_count').desc()).limit(15))\n",
    "\n",
    "print(\"\\n\\nüì¶ BASKET SIZE DISTRIBUTION:\")\n",
    "display(results['compositions'])\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"Analysis complete! Use the temp views for further exploration.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a958fee7-5104-44fa-9f87-94abf1fb3809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Association_analysis_2_triplets_quadruplets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
